{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load_word2vec_format('./gn/GoogleNews-vectors-negative300.bin', binary=True)  # C binary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stoplist = ['.', ',', ':', '\"', '?', '!', '<', '>', '(', ')', '@', '#', '$', '^', '*', '/', '\\n', ' ', '-', '_', 'a', 'the']\n",
    "def tokenize(string, tokens):\n",
    "    \"\"\"\n",
    "    Разбивает входную строку на токены\n",
    "    :type string: str\n",
    "    :param string: строка, которая будет разбита на токены\n",
    "    :type tokens: list\n",
    "    :param tokens: список токенов\n",
    "    \"\"\"\n",
    "    current_word = str('')\n",
    "    for char in string:\n",
    "        if char.isalpha() and char not in stoplist:\n",
    "            current_word += char\n",
    "            continue\n",
    "        else:\n",
    "            if len(current_word) > 0:\n",
    "                if current_word.lower() not in tokens and current_word not in stoplist:\n",
    "                    tokens.append(current_word.lower())\n",
    "            if char.isalpha() and char not in stoplist:\n",
    "                current_word = char\n",
    "            else:\n",
    "                current_word = str('')\n",
    "\n",
    "    if len(current_word) > 0:\n",
    "        if current_word.lower() not in tokens and current_word not in stoplist:\n",
    "            tokens.append(current_word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Paragraph1 = 'A single Kingdom of Great Britain resulted from the Union of Scotland and England (which already comprised the present-day countries of England and Wales) in 1707'\n",
    "par1 = []\n",
    "tokenize(Paragraph1, par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Paragraph2 = 'Great Britain refers geographically to the island of Great Britain, politically to England, Scotland and Wales in combination.[27] However, it is sometimes used loosely to refer to the whole of the United Kingdom'\n",
    "par2 = []\n",
    "tokenize(Paragraph2, par2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def paragraph_vec(paragraph):\n",
    "    \"\"\"\n",
    "    Подсчитывает сумму векторов слов абзаца\n",
    "    :type paragraph: list of strings\n",
    "    :param paragraph: список всех слов абзаца\n",
    "    :return: list 300-vector\n",
    "    \"\"\"\n",
    "    par_vec = []\n",
    "    for x in xrange(300):\n",
    "        res = 0.0\n",
    "        error = 0.0\n",
    "        for word in paragraph:\n",
    "            if word in model:\n",
    "                y = model[word][x] - error\n",
    "                t = res + y\n",
    "                error = (t - res) - y\n",
    "                res = t\n",
    "        par_vec.append(res)\n",
    "    return par_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "par1_vec = paragraph_vec(par1)\n",
    "par2_vec = paragraph_vec(par2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(vec1, vec2):\n",
    "    len1 = norm(vec1)\n",
    "    len2 = norm(vec2)\n",
    "    if len1 == 0 or len2 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        cos = scalar_prod(vec1, vec2)/(len1*len2)\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def scalar_prod(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Подсчитывает скалярное произведение двух векторов\n",
    "    :type vec1: list\n",
    "    :param vec1: 300-вектор первого слова\n",
    "    :type vec2: list\n",
    "    :param vec2: 300-вектор второго слова\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def norm(vec):\n",
    "    return sqrt(scalar_prod(vec, vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829489248835\n"
     ]
    }
   ],
   "source": [
    "cos = cosine(par1_vec, par2_vec)\n",
    "print cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_cosines(text1, text2):\n",
    "    \"\"\"\n",
    "    Подсчитывает максимальное косинусное расстоение между словом первого абзаца и всеми словами второго.\n",
    "    Затем усредняет все эти значения.\n",
    "    :type text1: list\n",
    "    :param text1: список всех слов первого текста\n",
    "    :type text2: list\n",
    "    :param text2: список всех слов второго текста\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    max_cosines = [max([cosine(model[w1], model[w2]) for w2 in text2 if w2 in model]) for w1 in text1 if w1 in model] \n",
    "    av = sum(max_cosines)/len(max_cosines)\n",
    "    return av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = open(\"gutenberg_task.txt\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_line(line):\n",
    "    \"\"\"\n",
    "    Разделяет текст по знакам табуляции и сохраняет в список\n",
    "    :type line: string\n",
    "    :param line: текст\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    sents = []\n",
    "    first_sym_num = 0\n",
    "    last_sym_num = 0\n",
    "    for symbol in line:\n",
    "        last_sym_num += 1\n",
    "        if symbol == '\\t':\n",
    "            word = line[first_sym_num:last_sym_num]\n",
    "            sents.append(word)\n",
    "            first_sym_num = last_sym_num\n",
    "    word = line[first_sym_num:]\n",
    "    sents.append(word)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "for line in dataset:\n",
    "    text.append(split_line(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res1 = []\n",
    "res2 = []\n",
    "i = 0\n",
    "for paragraph in text:\n",
    "    if i % 50 == 0:\n",
    "        print i\n",
    "    sent1 = paragraph[0]\n",
    "    sent2 = paragraph[1]\n",
    "    sent3 = paragraph[2]\n",
    "    sent1_list = []\n",
    "    sent2_list = []\n",
    "    sent3_list = []\n",
    "    tokenize(sent1, sent1_list)\n",
    "    tokenize(sent2, sent2_list)\n",
    "    tokenize(sent3, sent3_list)\n",
    "    sent1_vec = paragraph_vec(sent1_list)\n",
    "    sent2_vec = paragraph_vec(sent2_list)\n",
    "    sent3_vec = paragraph_vec(sent3_list)\n",
    "    res1_par = [] \n",
    "    res2_par = []\n",
    "    res1_par.append(cosine(sent1_vec, sent2_vec))\n",
    "    res1_par.append(cosine(sent1_vec, sent3_vec))\n",
    "    res1.append(res1_par)\n",
    "    res2_par.append(max_cosines(sent1_list, sent2_list))\n",
    "    res2_par.append(max_cosines(sent1_list, sent3_list))\n",
    "    res2.append(res2_par)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1 = np.array(map(lambda x: int(x[0]>x[1]), res1))\n",
    "l2 = np.array(map(lambda x: int(x[0]>x[1]), res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность первого алгоритма: 0.6694\n",
      "Точность второго алгоритма: 0.6091\n",
      "Доверительный интервал первого алгоритма: (0.66017958751777339, 0.6786204124822266)\n",
      "Доверительный интервал второго алгоритма: (0.59953614008308354, 0.61866385991691641)\n"
     ]
    }
   ],
   "source": [
    "#Доверительные интервалы уровня доверия 0,95. нужная квантиль - 1,96\n",
    "m1 = l1.mean()\n",
    "m2 = l2.mean()\n",
    "d1 = 1.96*sqrt(m1*(1-m1)/len(l1))\n",
    "d2 = 1.96*sqrt(m2*(1-m2)/len(l2))\n",
    "print \"Точность первого алгоритма:\", m1\n",
    "print \"Точность второго алгоритма:\", m2\n",
    "print \"Доверительный интервал первого алгоритма:\", (m1-d1, m1+d1)\n",
    "print \"Доверительный интервал второго алгоритма:\", (m2-d2, m2+d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
