{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Рассмотрим датасет, который устроен так: в каждой строчке три абзаца, первые два взяты из одной книжки, третий из другой. \"Правильный\" ответ всегда на втором месте, \"неправильный\" на третьем.\n",
    "Всего строчек 10000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что уже есть:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. Найдены два варианта уже обученной модели:\n",
    "    1) записанная в бинарном формате. Она оказалась слишком большой для оперативной памяти моего компьютера. С ней можно поработать позже, когда появится доступ к более мощному компьютеру (после 7 февраля).\n",
    "    2) записанная в текстовом формате. Она менее точная, т.к. в ней, напримре, присутствуют векторы для знаков препинания. Но на данном этапе работы можно использовать и ее.\n",
    "\n",
    "2. Реализованы два метода для сравнения текстов по темам:\n",
    "    1) для каждого абзаца суммируем составляющие его вектора, считаем косинусное между ними\n",
    "    2) для каждого слова первого абзаца вычисляем косинусное расстояние с каждым словом второго абзаца, берём минимум; эти числа усредняем для всех слов первого абзаца"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Сохраняем модель в виде словаря: где ключ -- слово, значение -- вектор, ему соответствующий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В дальнейшем, если решим использовать первую модель, этот код будет изменен, поэтому для обработки строки файла не будем создавать отдельную функцию, а обработаем все на месте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Открываем файл\n",
    "glove_model = open(\"glove.txt\", \"r\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства работы с данными напишем функцию, которая будет преобразовывать вектор в числовой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def list_to_float(list_char):\n",
    "    \"\"\"\n",
    "    Все элементы списка из текстовых делает числовыми\n",
    "    :type list_char: list\n",
    "    :param list_char: список с текстовыми элементами\n",
    "    :return: list числовой\n",
    "    \"\"\"\n",
    "    list_float = []\n",
    "    for elem in list_char:\n",
    "        list_float.append(float(elem))\n",
    "    return list_float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построчно считываем и сразу записываем в словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = {}\n",
    "stopwords = ['.', ',', ':', '\"', '?', '!', '<', '>', '(', ')', '@', '#', '$', '^', '*', '/', '_']\n",
    "for line in glove_model:\n",
    "    splitted_line = line.split(' ')\n",
    "    key = splitted_line.pop(0)\n",
    "    if key not in stopwords:\n",
    "        splitted_line = list_to_float(splitted_line)\n",
    "        model[key] = splitted_line\n",
    "line = glove_model.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь считываем сам текст из файла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('input.txt', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию для разбивания строк на токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stoplist = ['.', ',', ':', '\"', '?', '!', '<', '>', '(', ')', '@', '#', '$', '^', '*', '/', '\\n', ' ', '-', '_']\n",
    "def tokenize(string, tokens):\n",
    "    \"\"\"\n",
    "    Разбивает входную строку на токены\n",
    "    :type string: str\n",
    "    :param string: строка, которая будет разбита на токены\n",
    "    :type tokens: list\n",
    "    :param tokens: список токенов\n",
    "    \"\"\"\n",
    "    current_word = str('')\n",
    "    for char in string:\n",
    "        if char.isalpha() and char not in stoplist:\n",
    "            current_word += char\n",
    "            continue\n",
    "        else:\n",
    "            if len(current_word) > 0:\n",
    "                if current_word.lower() not in tokens and current_word not in stoplist:\n",
    "                    tokens.append(current_word.lower())\n",
    "            if char.isalpha() and char not in stoplist:\n",
    "                current_word = char\n",
    "            else:\n",
    "                current_word = str('')\n",
    "\n",
    "    if len(current_word) > 0:\n",
    "        if current_word.lower() not in tokens and current_word not in stoplist:\n",
    "            tokens.append(current_word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "for line in f:\n",
    "    tokenize(line, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого токена, который есть в модели, получаем из данных и сохраняем его вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectors = {}\n",
    "    \n",
    "for token in tokens:\n",
    "    if token in model:\n",
    "        vectors[token] = model[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Отладочный вывод\n",
    "#for key in vectors:\n",
    "#    print key, ':', vectors[key]\n",
    "#    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, суммирующую векторы всех слов абзаца. Сразу воспользуемся алгоритмом Кахана, чтобы уменьшить погрешность при суммировании.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def paragraph_vec(paragraph):\n",
    "    \"\"\"\n",
    "    Подсчитывает сумму векторов слов абзаца\n",
    "    :type paragraph: list of strings\n",
    "    :param paragraph: список всех слов абзаца\n",
    "    :return: list 300-vector\n",
    "    \"\"\"\n",
    "    par_vec = []\n",
    "    for x in xrange(300):\n",
    "        res = 0.0\n",
    "        error = 0.0\n",
    "        for word in paragraph:\n",
    "            if word in model:\n",
    "                y = model[word][x] - error\n",
    "                t = res + y\n",
    "                error = (t - res) - y\n",
    "                res = t\n",
    "        par_vec.append(res)\n",
    "    return par_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def par2vecwithNP (paragraph):\n",
    "    res = np.array([0.0]*300)\n",
    "    error = np.array([0.0]*300)\n",
    "    for word in paragraph:\n",
    "        if word in model:\n",
    "            y = np.array(model[word]) - error\n",
    "            t = res + y\n",
    "            error = (t - res) - y\n",
    "            res = t\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0764248371124\n",
      "0.0120670795441\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "res1 = []\n",
    "res2 = []\n",
    "st = time.time()\n",
    "for i in xrange(10):\n",
    "    res1.append(paragraph_vec(genPar[i]))\n",
    "print time.time()-st\n",
    "\n",
    "st = time.time()\n",
    "for i in xrange(10):\n",
    "    res2.append(par2vecwithNP(genPar[i]))\n",
    "print time.time()-st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для текущей модели скорее всего вполне подойдет и простое суммирование в лоб, но на всякий случай для подстраховки оставим еще функцию скалярного произведения, где используется алгоритм Кахана, даже если в итоге она не понадобится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12355.329097948295"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scalar_prod(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Подсчитывает скалярное произведение двух векторов\n",
    "    :type vec1: list\n",
    "    :param vec1: 300-вектор первого слова\n",
    "    :type vec2: list\n",
    "    :param vec2: 300-вектор второго слова\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    return np.dot(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12355.329097948295"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scalar_prod_kahan(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Подсчитывает скалярное произведение двух векторов\n",
    "    :type vec1: list\n",
    "    :param vec1: 300-вектор первого слова\n",
    "    :type vec2: list\n",
    "    :param vec2: 300-вектор второго слова\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    res = 0.0\n",
    "    error = 0.0\n",
    "    for x in xrange(300):\n",
    "        mult = vec1[x]*vec2[x]\n",
    "        y = mult - error\n",
    "        t = res + y\n",
    "        error = (t - res) - y\n",
    "        res = t\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.20338782697\n"
     ]
    }
   ],
   "source": [
    "print scalar_prod(model['hello'], model['world'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.20338782697\n"
     ]
    }
   ],
   "source": [
    "print scalar_prod_kahan(model['hello'], model['world'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def norm(vec):\n",
    "    return sqrt(scalar_prod(vec, vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine(vec1, vec2):\n",
    "    len1 = norm(vec1)\n",
    "    len2 = norm(vec2)\n",
    "    if len1 == 0 or len2 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        cos = scalar_prod(vec1, vec2)/(len1*len2)\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Paragraph1 = 'A single Kingdom of Great Britain resulted from the Union of Scotland and England (which already comprised the present-day countries of England and Wales) in 1707'\n",
    "par1 = []\n",
    "tokenize(Paragraph1, par1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Paragraph2 = 'Great Britain refers geographically to the island of Great Britain, politically to England, Scotland and Wales in combination.[27] However, it is sometimes used loosely to refer to the whole of the United Kingdom'\n",
    "par2 = []\n",
    "tokenize(Paragraph2, par2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взяли два произовольных абзаца, для каждого нашли его суммарный вектор. Посмотрим на косинусное между ними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91373987894\n"
     ]
    }
   ],
   "source": [
    "par1_vec = paragraph_vec(par1)\n",
    "par2_vec = paragraph_vec(par2)\n",
    "cos = cosine(par1_vec, par2_vec)\n",
    "print cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь для каждого слова первого абзаца вычисляем косинусное расстояние с каждым словом второго абзаца, берём минимум; эти числа усредняем для всех слов первого абзаца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_cosines(text1, text2):\n",
    "    \"\"\"\n",
    "    Подсчитывает максимальное косинусное расстоение между словом первого абзаца и всеми словами второго.\n",
    "    Затем усредняет все эти значения.\n",
    "    :type text1: list\n",
    "    :param text1: список всех слов первого текста\n",
    "    :type text2: list\n",
    "    :param text2: список всех слов второго текста\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    max_cosines = [max([cosine(model[w1], model[w2]) for w2 in text2 if w2 in model]) for w1 in text1 if w1 in model] \n",
    "    av = sum(max_cosines)/len(max_cosines)\n",
    "    return av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74766992635\n"
     ]
    }
   ],
   "source": [
    "print max_cosines(par1, par2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считываем датасет, который устроен так: в каждой строчке три абзаца, первые два взяты из одной книжки, третий из другой. \"Правильный\" ответ всегда на втором месте, \"неправильный\" на третьем.\n",
    "Всего строчек 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = open(\"gutenberg_task.txt\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_line(line):\n",
    "    \"\"\"\n",
    "    Разделяет текст по знакам табуляции и сохраняет в список\n",
    "    :type line: string\n",
    "    :param line: текст\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    sents = []\n",
    "    first_sym_num = 0\n",
    "    last_sym_num = 0\n",
    "    for symbol in line:\n",
    "        last_sym_num += 1\n",
    "        if symbol == '\\t':\n",
    "            word = line[first_sym_num:last_sym_num]\n",
    "            sents.append(word)\n",
    "            first_sym_num = last_sym_num\n",
    "    word = line[first_sym_num:]\n",
    "    sents.append(word)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "for line in dataset:\n",
    "    text.append(split_line(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res1 = []\n",
    "res2 = []\n",
    "i = 0\n",
    "for paragraph in text:\n",
    "    if i % 50 == 0:\n",
    "        print i\n",
    "    sent1 = paragraph[0]\n",
    "    sent2 = paragraph[1]\n",
    "    sent3 = paragraph[2]\n",
    "    sent1_list = []\n",
    "    sent2_list = []\n",
    "    sent3_list = []\n",
    "    tokenize(sent1, sent1_list)\n",
    "    tokenize(sent2, sent2_list)\n",
    "    tokenize(sent3, sent3_list)\n",
    "    sent1_vec = paragraph_vec(sent1_list)\n",
    "    sent2_vec = paragraph_vec(sent2_list)\n",
    "    sent3_vec = paragraph_vec(sent3_list)\n",
    "    res1_par = [] \n",
    "    res2_par = []\n",
    "    res1_par.append(cosine(sent1_vec, sent2_vec))\n",
    "    res1_par.append(cosine(sent1_vec, sent3_vec))\n",
    "    res1.append(res1_par)\n",
    "    res2_par.append(max_cosines(sent1_list, sent2_list))\n",
    "    res2_par.append(max_cosines(sent1_list, sent3_list))\n",
    "    res2.append(res2_par)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитаем точности алгоритмов и их доверительные интервалы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1 = np.array(map(lambda x: int(x[0]>x[1]), res1))\n",
    "l2 = np.array(map(lambda x: int(x[0]>x[1]), res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность первого алгоритма: 0.7356\n",
      "Точность второго алгоритма: 0.666\n",
      "Доверительный интервал первого алгоритма: (0.72695614133720365, 0.74424385866279641)\n",
      "Доверительный интервал второго алгоритма: (0.65675586201747294, 0.67524413798252714)\n"
     ]
    }
   ],
   "source": [
    "#Доверительные интервалы уровня доверия 0,95. нужная квантиль - 1,96\n",
    "m1 = l1.mean()\n",
    "m2 = l2.mean()\n",
    "d1 = 1.96*sqrt(m1*(1-m1)/len(l1))\n",
    "d2 = 1.96*sqrt(m2*(1-m2)/len(l2))\n",
    "print \"Точность первого алгоритма:\", m1\n",
    "print \"Точность второго алгоритма:\", m2\n",
    "print \"Доверительный интервал первого алгоритма:\", (m1-d1, m1+d1)\n",
    "print \"Доверительный интервал второго алгоритма:\", (m2-d2, m2+d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что первый алгоритм обладает большей точностью."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
